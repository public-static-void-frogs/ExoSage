{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74bff43d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/admin/Documents/code/nasa_exoplanets/.venv/lib/python3.13/site-packages/lightkurve/prf/__init__.py:7: UserWarning: Warning: the tpfmodel submodule is not available without oktopus installed, which requires a current version of autograd. See #1452 for details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from astropy.io import ascii\n",
    "import matplotlib.pyplot as plt\n",
    "import lightkurve as lk\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1890d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess_lc(lc):\n",
    "#     lc_cleaned = lc.remove_outliers()\n",
    "#     lc_normalized = lc_cleaned / lc_cleaned.flux.max()\n",
    "#     lc_normalized.time = (lc_normalized.time - lc_normalized.time[0]).value\n",
    "#     return lc_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df93dc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "TABLES_FOLDER = \"./tbls_not_in_koi\"\n",
    "folder = Path(TABLES_FOLDER)\n",
    "tbl_files = folder.iterdir()\n",
    "num_timesteps = 72_000\n",
    "koi = pd.read_csv('koi_last_cumulative.csv', comment='#')\n",
    "SAVE_FOLDER = './init_df_not_in_koi/'\n",
    "\n",
    "ID_DF_NAME = SAVE_FOLDER + 'df_version_02_id'\n",
    "VALUE_DF_NAME = SAVE_FOLDER + 'df_version_02_value'\n",
    "APPENDING = False\n",
    "SAVING_INTERVAL = 500\n",
    "STARTING_INDEX = 0\n",
    "number_of_tables = len(os.listdir(TABLES_FOLDER))\n",
    "TBL_CURVE_COLUMN = 'LC_INIT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f97cf40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "# # read already processed files\n",
    "# processed_set = set()\n",
    "# if Path(\"already_processed_tbls.txt\").exists():\n",
    "#     with open(\"already_processed_tbls.txt\") as f:\n",
    "#         processed_set = set(line.strip() for line in f if line.strip())\n",
    "\n",
    "# # build tbl_files as a list, excluding already processed\n",
    "# tbl_files = [f for f in Path(\"your_ascii_folder\").iterdir() \n",
    "#              if f.is_file() and str(f) not in processed_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75890bdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8496, 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# updating the TABLES_FOLDER compound table\n",
    "data = []\n",
    "for f in folder.iterdir():\n",
    "    if f.is_file():\n",
    "        kepid_m = re.search(r'kplr(\\d+)_', str(f))\n",
    "        tce_m = re.search(r'tce_(\\d+)_', str(f))\n",
    "        if kepid_m and tce_m:\n",
    "            data.append((int(kepid_m.group(1)), int(tce_m.group(1)), f.name))\n",
    "tbls_compound = pd.DataFrame(data, columns=['kepid','ascii_planet_num','file_name']).set_index(['kepid','ascii_planet_num'])\n",
    "tbls_compound.to_csv(f'{TABLES_FOLDER}_compound_key.csv')\n",
    "tbls_compound.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8165953",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(346)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "koi['koi_tce_plnt_num'].isna().sum() #how many planet nums are not set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e27a08a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bc we use composite key where second element is a serial number of the tce\n",
    "# we must ensure all serial numbers exist\n",
    "# here we find star systems which have multiple planets, all NaN\n",
    "koi_no_plnt_num = koi[koi['koi_tce_plnt_num'].isna()]\n",
    "a = koi_no_plnt_num['kepid'].to_list()\n",
    "def find_recurring_elements(lst):\n",
    "    counts = pd.Series(lst).value_counts()\n",
    "    return counts[counts > 1].index.tolist()\n",
    "nonmarked_stars_mltpl_planets = find_recurring_elements(a)\n",
    "\n",
    "#drop systems where there are multiple planets and they are unmarked bc we can't map them to curves\n",
    "# leave koi that have planet number OR are the only tce in star system\n",
    "koi = koi[(koi['koi_tce_plnt_num'].notna()) | (~koi['kepid'].isin(nonmarked_stars_mltpl_planets))]\n",
    "koi['koi_tce_plnt_num'] = koi['koi_tce_plnt_num'].fillna(1)\n",
    "koi['koi_tce_plnt_num'] = koi['koi_tce_plnt_num'].apply(int)\n",
    "\n",
    "#we need compound keys but drop dups for now\n",
    "koi_compound = koi.set_index(['kepid', 'koi_tce_plnt_num'])\n",
    "koi_compound = koi_compound.drop(koi_compound[koi_compound.index.duplicated()].index, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57861b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('label_mapping.json', 'r') as file:\n",
    "    label_mapping = json.load(file)\n",
    "label_map = label_mapping['label_map']\n",
    "next_code = label_mapping['next_code']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ccac7ea3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CONFIRMED': 0, 'CANDIDATE': 1, 'FALSE POSITIVE': 2, 'TRUE NEGATIVE': 3}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6b7c7a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 0 entries\n",
      "scanned 100 tables out of 8497\n",
      "found 100 entries\n",
      "scanned 200 tables out of 8497\n",
      "found 200 entries\n",
      "scanned 300 tables out of 8497\n",
      "found 300 entries\n",
      "scanned 400 tables out of 8497\n",
      "found 400 entries\n",
      "scanned 500 tables out of 8497\n",
      "saving: 1\n",
      "found 500 entries\n",
      "scanned 600 tables out of 8497\n",
      "found 600 entries\n",
      "scanned 700 tables out of 8497\n",
      "found 700 entries\n",
      "scanned 800 tables out of 8497\n",
      "found 800 entries\n",
      "scanned 900 tables out of 8497\n",
      "found 900 entries\n",
      "scanned 1000 tables out of 8497\n",
      "saving: 2\n",
      "found 1000 entries\n",
      "scanned 1100 tables out of 8497\n",
      "found 1100 entries\n",
      "scanned 1200 tables out of 8497\n",
      "found 1200 entries\n",
      "scanned 1300 tables out of 8497\n",
      "found 1300 entries\n",
      "scanned 1400 tables out of 8497\n",
      "found 1400 entries\n",
      "scanned 1500 tables out of 8497\n",
      "saving: 3\n",
      "found 1500 entries\n",
      "scanned 1600 tables out of 8497\n",
      "found 1600 entries\n",
      "scanned 1700 tables out of 8497\n",
      "found 1700 entries\n",
      "scanned 1800 tables out of 8497\n",
      "found 1800 entries\n",
      "scanned 1900 tables out of 8497\n",
      "found 1900 entries\n",
      "scanned 2000 tables out of 8497\n",
      "saving: 4\n",
      "found 2000 entries\n",
      "scanned 2100 tables out of 8497\n",
      "found 2100 entries\n",
      "scanned 2200 tables out of 8497\n",
      "found 2200 entries\n",
      "scanned 2300 tables out of 8497\n",
      "found 2300 entries\n",
      "scanned 2400 tables out of 8497\n",
      "found 2400 entries\n",
      "scanned 2500 tables out of 8497\n",
      "saving: 5\n",
      "found 2500 entries\n",
      "scanned 2600 tables out of 8497\n",
      "found 2600 entries\n",
      "scanned 2700 tables out of 8497\n",
      "found 2700 entries\n",
      "scanned 2800 tables out of 8497\n",
      "found 2800 entries\n",
      "scanned 2900 tables out of 8497\n",
      "found 2900 entries\n",
      "scanned 3000 tables out of 8497\n",
      "saving: 6\n",
      "found 3000 entries\n",
      "scanned 3100 tables out of 8497\n",
      "found 3100 entries\n",
      "scanned 3200 tables out of 8497\n",
      "found 3200 entries\n",
      "scanned 3300 tables out of 8497\n",
      "found 3300 entries\n",
      "scanned 3400 tables out of 8497\n",
      "found 3400 entries\n",
      "scanned 3500 tables out of 8497\n",
      "saving: 7\n",
      "found 3500 entries\n",
      "scanned 3600 tables out of 8497\n",
      "found 3600 entries\n",
      "scanned 3700 tables out of 8497\n",
      "found 3700 entries\n",
      "scanned 3800 tables out of 8497\n",
      "found 3800 entries\n",
      "scanned 3900 tables out of 8497\n",
      "found 3900 entries\n",
      "scanned 4000 tables out of 8497\n",
      "saving: 8\n",
      "found 4000 entries\n",
      "ERROR for file tbls_not_in_koi/search_14666024.log, continuing\n",
      "scanned 4100 tables out of 8497\n",
      "found 4100 entries\n",
      "scanned 4200 tables out of 8497\n",
      "found 4200 entries\n",
      "scanned 4300 tables out of 8497\n",
      "found 4300 entries\n",
      "scanned 4400 tables out of 8497\n",
      "found 4400 entries\n",
      "scanned 4500 tables out of 8497\n",
      "saving: 9\n",
      "found 4500 entries\n",
      "scanned 4600 tables out of 8497\n",
      "found 4600 entries\n",
      "scanned 4700 tables out of 8497\n",
      "found 4700 entries\n",
      "scanned 4800 tables out of 8497\n",
      "found 4800 entries\n",
      "scanned 4900 tables out of 8497\n",
      "found 4900 entries\n",
      "scanned 5000 tables out of 8497\n",
      "saving: 10\n",
      "found 5000 entries\n",
      "scanned 5100 tables out of 8497\n",
      "found 5100 entries\n",
      "scanned 5200 tables out of 8497\n",
      "found 5200 entries\n",
      "scanned 5300 tables out of 8497\n",
      "found 5300 entries\n",
      "scanned 5400 tables out of 8497\n",
      "found 5400 entries\n",
      "scanned 5500 tables out of 8497\n",
      "saving: 11\n",
      "found 5500 entries\n",
      "scanned 5600 tables out of 8497\n",
      "found 5600 entries\n",
      "scanned 5700 tables out of 8497\n",
      "found 5700 entries\n",
      "scanned 5800 tables out of 8497\n",
      "found 5800 entries\n",
      "scanned 5900 tables out of 8497\n",
      "found 5900 entries\n",
      "scanned 6000 tables out of 8497\n",
      "saving: 12\n",
      "found 6000 entries\n",
      "scanned 6100 tables out of 8497\n",
      "found 6100 entries\n",
      "scanned 6200 tables out of 8497\n",
      "found 6200 entries\n",
      "scanned 6300 tables out of 8497\n",
      "found 6300 entries\n",
      "scanned 6400 tables out of 8497\n",
      "found 6400 entries\n",
      "scanned 6500 tables out of 8497\n",
      "saving: 13\n",
      "found 6500 entries\n",
      "scanned 6600 tables out of 8497\n",
      "found 6600 entries\n",
      "scanned 6700 tables out of 8497\n",
      "found 6700 entries\n",
      "scanned 6800 tables out of 8497\n",
      "found 6800 entries\n",
      "scanned 6900 tables out of 8497\n",
      "found 6900 entries\n",
      "scanned 7000 tables out of 8497\n",
      "saving: 14\n",
      "found 7000 entries\n",
      "scanned 7100 tables out of 8497\n",
      "found 7100 entries\n",
      "scanned 7200 tables out of 8497\n",
      "found 7200 entries\n",
      "scanned 7300 tables out of 8497\n",
      "found 7300 entries\n",
      "scanned 7400 tables out of 8497\n",
      "found 7400 entries\n",
      "scanned 7500 tables out of 8497\n",
      "saving: 15\n",
      "found 7500 entries\n",
      "scanned 7600 tables out of 8497\n",
      "found 7600 entries\n",
      "scanned 7700 tables out of 8497\n",
      "found 7700 entries\n",
      "scanned 7800 tables out of 8497\n",
      "found 7800 entries\n",
      "scanned 7900 tables out of 8497\n",
      "found 7900 entries\n",
      "scanned 8000 tables out of 8497\n",
      "saving: 16\n",
      "found 8000 entries\n",
      "scanned 8100 tables out of 8497\n",
      "found 8100 entries\n",
      "scanned 8200 tables out of 8497\n",
      "found 8200 entries\n",
      "scanned 8300 tables out of 8497\n",
      "found 8300 entries\n",
      "scanned 8400 tables out of 8497\n",
      "found 8400 entries\n"
     ]
    }
   ],
   "source": [
    "#ITERATION OVER TABLES\n",
    "processed_lcs = []\n",
    "kepids = []\n",
    "planet_nums = []\n",
    "labels = []\n",
    "i_saved = STARTING_INDEX #APPENDED IN TOTAL\n",
    "i_tbls = 0 #TBLS GONE OVER\n",
    "just_saved_flag = True\n",
    "for tbl_file in tbl_files:\n",
    "    i_tbls += 1\n",
    "    #progress\n",
    "    if i_tbls % 100 == 0:\n",
    "        print(f'scanned {i_tbls} tables out of {number_of_tables}')\n",
    "\n",
    "    # saving\n",
    "    if i_saved % SAVING_INTERVAL == 0 and i_saved != 0 and not just_saved_flag:\n",
    "        # if i_saved % 100 == 0:\n",
    "        #     print(f'found {i_saved} entries')\n",
    "        print(f'saving: {i_saved//SAVING_INTERVAL}')\n",
    "        lcs_df = pd.DataFrame(processed_lcs)\n",
    "        \n",
    "        df_ids = pd.DataFrame({'LABEL': labels, 'KEPID': kepids, 'PLANET_NUM' : planet_nums})\n",
    "        df_ids.to_csv(ID_DF_NAME + f'_{i_saved // SAVING_INTERVAL}.csv', index=False)\n",
    "        #lcs_df = pd.concat((identity_df, lcs_df))\n",
    "        \n",
    "        #df_ids = lcs_df[['KEPID','PLANET_NUM','LABEL']].copy()\n",
    "        lcs_df.columns = lcs_df.columns.astype(str)\n",
    "        df_values = lcs_df[[str(i) for i in range(0,num_timesteps)]].T\n",
    "        \n",
    "        df_values.columns = df_values.columns.astype(str)\n",
    "        #df_values = lcs_df[[str(i) for i in range(0,70000)]].T\n",
    "        df_values.reset_index(drop=True, inplace=True)\n",
    "        df_values.columns = [str(i) for i in range(df_values.shape[1])]\n",
    "        df_values.to_parquet(VALUE_DF_NAME + f'_{i_saved // SAVING_INTERVAL}.parquet', engine=\"fastparquet\", compression=\"snappy\")\n",
    "\n",
    "        processed_lcs = []\n",
    "        kepids = []\n",
    "        planet_nums = []\n",
    "        labels = []\n",
    "\n",
    "        just_saved_flag = True\n",
    "\n",
    "    #identification\n",
    "    s = re.search(r'kplr(\\d+)_', str(tbl_file))\n",
    "    if s is not None:\n",
    "        kepid = int(s.group(1))\n",
    "        ascii_planet_num = int(re.search(r'tce_(\\d+)_', str(tbl_file)).group(1))\n",
    "    else:\n",
    "        print(f'ERROR for file {str(tbl_file)}, continuing')\n",
    "        continue\n",
    "\n",
    "    # append if it's not in koi\n",
    "    if (kepid, ascii_planet_num) not in koi_compound.index: #CHANGED TO APPEND ONLY THOSE NOT IN KOI\n",
    "        if i_saved % 100 == 0:\n",
    "            print(f'found {i_saved} entries')\n",
    "        tbl = ascii.read(tbl_file)\n",
    "        # df = tbl.to_pandas()[[\"TIME\", TBL_CURVE_COLUMN]]\n",
    "        # lc = lk.LightCurve(time=df['TIME'], flux=df[TBL_CURVE_COLUMN])\n",
    "        # processed_lc = preprocess_lc(lc)\n",
    "        # flux = processed_lc.flux.value\n",
    "        flux = tbl.to_pandas()[TBL_CURVE_COLUMN].to_list() # WE SKIP LK PROCESSING AND TAKE THE RAW LC_INIT\n",
    "        if len(flux) < num_timesteps:\n",
    "            #print('padding:', num_timesteps - len(flux))\n",
    "            flux = np.pad(flux, (0, num_timesteps - len(flux)), constant_values=np.nan)\n",
    "        elif len(flux) > num_timesteps:\n",
    "            print('truncating:', num_timesteps - len(flux))\n",
    "            flux = flux[:num_timesteps]\n",
    "\n",
    "        # HERE TOO\n",
    "        #label = koi_compound.loc[(kepid, ascii_planet_num)]['koi_disposition']\n",
    "        label = \"TRUE NEGATIVE\"\n",
    "\n",
    "        if label not in label_map:\n",
    "            label_map[label] = next_code\n",
    "            next_code += 1\n",
    "\n",
    "        processed_lcs.append(flux)\n",
    "        kepids.append(kepid)\n",
    "        planet_nums.append(ascii_planet_num)\n",
    "        labels.append(label_map[label])\n",
    "        with open(f\"{SAVE_FOLDER}already_processed_tbls.txt\", \"a\") as f:\n",
    "            f.write(str(tbl_file) + \"\\n\") # TODO: IMPLEMENT ITS USAGE\n",
    "        i_saved += 1\n",
    "        if just_saved_flag: just_saved_flag = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2cfbcdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "koi_compound = koi_compound.iloc[STARTING_INDEX:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bcb0d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "went through 100 entries in KOI table out of 9360\n",
      "went through 200 entries in KOI table out of 9360\n",
      "went through 300 entries in KOI table out of 9360\n",
      "went through 400 entries in KOI table out of 9360\n",
      "went through 500 entries in KOI table out of 9360\n",
      "found 500 entries\n",
      "saving: 500\n",
      "went through 600 entries in KOI table out of 9360\n",
      "went through 700 entries in KOI table out of 9360\n",
      "went through 800 entries in KOI table out of 9360\n",
      "went through 900 entries in KOI table out of 9360\n",
      "went through 1000 entries in KOI table out of 9360\n",
      "found 1000 entries\n",
      "saving: 1000\n",
      "went through 1100 entries in KOI table out of 9360\n",
      "went through 1200 entries in KOI table out of 9360\n",
      "went through 1300 entries in KOI table out of 9360\n",
      "went through 1400 entries in KOI table out of 9360\n",
      "went through 1500 entries in KOI table out of 9360\n",
      "found 1500 entries\n",
      "saving: 1500\n",
      "went through 1600 entries in KOI table out of 9360\n",
      "went through 1700 entries in KOI table out of 9360\n",
      "went through 1800 entries in KOI table out of 9360\n",
      "went through 1900 entries in KOI table out of 9360\n",
      "went through 2000 entries in KOI table out of 9360\n",
      "found 2000 entries\n",
      "saving: 2000\n",
      "went through 2100 entries in KOI table out of 9360\n",
      "went through 2200 entries in KOI table out of 9360\n",
      "went through 2300 entries in KOI table out of 9360\n",
      "went through 2400 entries in KOI table out of 9360\n",
      "went through 2500 entries in KOI table out of 9360\n",
      "found 2500 entries\n",
      "saving: 2500\n",
      "went through 2600 entries in KOI table out of 9360\n",
      "went through 2700 entries in KOI table out of 9360\n",
      "went through 2800 entries in KOI table out of 9360\n",
      "went through 2900 entries in KOI table out of 9360\n",
      "went through 3000 entries in KOI table out of 9360\n",
      "found 3000 entries\n",
      "saving: 3000\n",
      "went through 3100 entries in KOI table out of 9360\n",
      "went through 3200 entries in KOI table out of 9360\n",
      "went through 3300 entries in KOI table out of 9360\n",
      "went through 3400 entries in KOI table out of 9360\n",
      "went through 3500 entries in KOI table out of 9360\n",
      "found 3500 entries\n",
      "saving: 3500\n",
      "went through 3600 entries in KOI table out of 9360\n",
      "went through 3700 entries in KOI table out of 9360\n",
      "went through 3800 entries in KOI table out of 9360\n",
      "went through 3900 entries in KOI table out of 9360\n",
      "went through 4000 entries in KOI table out of 9360\n",
      "found 4000 entries\n",
      "saving: 4000\n",
      "went through 4100 entries in KOI table out of 9360\n",
      "went through 4200 entries in KOI table out of 9360\n",
      "went through 4300 entries in KOI table out of 9360\n",
      "went through 4400 entries in KOI table out of 9360\n",
      "went through 4500 entries in KOI table out of 9360\n",
      "went through 4600 entries in KOI table out of 9360\n",
      "found 4500 entries\n",
      "saving: 4500\n",
      "went through 4700 entries in KOI table out of 9360\n",
      "went through 4800 entries in KOI table out of 9360\n",
      "went through 4900 entries in KOI table out of 9360\n",
      "went through 5000 entries in KOI table out of 9360\n",
      "went through 5100 entries in KOI table out of 9360\n",
      "found 5000 entries\n",
      "saving: 5000\n",
      "went through 5200 entries in KOI table out of 9360\n",
      "went through 5300 entries in KOI table out of 9360\n",
      "went through 5400 entries in KOI table out of 9360\n",
      "went through 5500 entries in KOI table out of 9360\n",
      "went through 5600 entries in KOI table out of 9360\n",
      "found 5500 entries\n",
      "saving: 5500\n",
      "went through 5700 entries in KOI table out of 9360\n",
      "went through 5800 entries in KOI table out of 9360\n",
      "went through 5900 entries in KOI table out of 9360\n",
      "went through 6000 entries in KOI table out of 9360\n",
      "went through 6100 entries in KOI table out of 9360\n",
      "went through 6200 entries in KOI table out of 9360\n",
      "went through 6300 entries in KOI table out of 9360\n",
      "went through 6400 entries in KOI table out of 9360\n",
      "went through 6500 entries in KOI table out of 9360\n",
      "went through 6600 entries in KOI table out of 9360\n",
      "found 6000 entries\n",
      "saving: 6000\n",
      "went through 6700 entries in KOI table out of 9360\n",
      "went through 6800 entries in KOI table out of 9360\n",
      "went through 6900 entries in KOI table out of 9360\n",
      "went through 7000 entries in KOI table out of 9360\n",
      "went through 7100 entries in KOI table out of 9360\n",
      "went through 7200 entries in KOI table out of 9360\n",
      "went through 7300 entries in KOI table out of 9360\n",
      "went through 7400 entries in KOI table out of 9360\n",
      "found 6500 entries\n",
      "saving: 6500\n",
      "went through 7500 entries in KOI table out of 9360\n",
      "went through 7600 entries in KOI table out of 9360\n",
      "went through 7700 entries in KOI table out of 9360\n",
      "went through 7800 entries in KOI table out of 9360\n",
      "went through 7900 entries in KOI table out of 9360\n",
      "went through 8000 entries in KOI table out of 9360\n",
      "found 7000 entries\n",
      "saving: 7000\n",
      "went through 8100 entries in KOI table out of 9360\n",
      "went through 8200 entries in KOI table out of 9360\n",
      "went through 8300 entries in KOI table out of 9360\n",
      "went through 8400 entries in KOI table out of 9360\n",
      "went through 8500 entries in KOI table out of 9360\n",
      "went through 8600 entries in KOI table out of 9360\n",
      "found 7500 entries\n",
      "saving: 7500\n",
      "went through 8700 entries in KOI table out of 9360\n",
      "went through 8800 entries in KOI table out of 9360\n",
      "went through 8900 entries in KOI table out of 9360\n",
      "went through 9000 entries in KOI table out of 9360\n",
      "went through 9100 entries in KOI table out of 9360\n",
      "found 8000 entries\n",
      "saving: 8000\n",
      "went through 9200 entries in KOI table out of 9360\n",
      "went through 9300 entries in KOI table out of 9360\n"
     ]
    }
   ],
   "source": [
    "# # GO OVER KOI\n",
    "# processed_lcs = []\n",
    "# kepids = []\n",
    "# planet_nums = []\n",
    "# labels = []\n",
    "# i_koi = STARTING_INDEX\n",
    "# i_saved = STARTING_INDEX\n",
    "# just_saved_flag = True\n",
    "# #iterating over koi csv\n",
    "# for koi_idx, koi_data in koi_compound.iterrows():\n",
    "#     i_koi += 1\n",
    "#     if i_koi % 100 == 0:\n",
    "#         print(f'went through {i_koi} entries in KOI table out of {koi_compound.shape[0]}')\n",
    "#     # if we have this curve file\n",
    "#     if koi_idx in tbls_compound.index:\n",
    "#         # we read and process the curve\n",
    "#         try:\n",
    "#             tbl = ascii.read(TABLES_FOLDER +  tbls_compound.loc[koi_idx]['file_name'])\n",
    "#             #print(f'read file {tbls_compound.loc[koi_idx]['file_name']}')\n",
    "#         except:\n",
    "#             print(f'couldnt read table {tbls_compound.loc[koi_idx]['file_name']}')\n",
    "#         # df = tbl.to_pandas()[[\"TIME\", TBL_CURVE_COLUMN]]\n",
    "#         # lc = lk.LightCurve(time=df['TIME'], flux=df[TBL_CURVE_COLUMN])\n",
    "#         # processed_lc = preprocess_lc(lc)\n",
    "#         # flux = processed_lc.flux.value\n",
    "#         flux = tbl.to_pandas()[TBL_CURVE_COLUMN].to_list() # WE SKIP LK PROCESSING AND TAKE THE RAW LC_INIT\n",
    "#         if len(flux) < num_timesteps:\n",
    "#             #print('padding:', num_timesteps - len(flux))\n",
    "#             flux = np.pad(flux, (0, num_timesteps - len(flux)), constant_values=np.nan)\n",
    "#         elif len(flux) > num_timesteps:\n",
    "#             print('truncating:', num_timesteps - len(flux))\n",
    "#             flux = flux[:num_timesteps]\n",
    "\n",
    "#         # get and save identification\n",
    "#         label = koi_data['koi_disposition']\n",
    "#         kepid, ascii_planet_num = koi_idx\n",
    "\n",
    "#         if label not in label_map:\n",
    "#             label_map[label] = next_code\n",
    "#             next_code += 1\n",
    "\n",
    "#         processed_lcs.append(flux)\n",
    "#         kepids.append(kepid)\n",
    "#         planet_nums.append(ascii_planet_num)\n",
    "#         labels.append(label_map[label])\n",
    "#         i_saved += 1\n",
    "#         if just_saved_flag: just_saved_flag = False\n",
    "\n",
    "#     if i_saved % SAVING_INTERVAL == 0 and not just_saved_flag:\n",
    "#         if i_saved % 100 == 0:\n",
    "#             print(f'found {i_saved} entries')\n",
    "#         print(f'saving: {i_saved}')\n",
    "#         lcs_df = pd.DataFrame(processed_lcs)\n",
    "#         identity_df = pd.DataFrame({'LABEL': labels, 'KEPID': kepids, 'PLANET_NUM' : planet_nums})\n",
    "#         #lcs_df = pd.concat((identity_df, lcs_df))\n",
    "        \n",
    "#         identity_df.to_csv(ID_DF_NAME + f'_{i_saved // SAVING_INTERVAL}.csv', index=False)\n",
    "#         lcs_df.columns = lcs_df.columns.astype(str)\n",
    "#         df_values = lcs_df[[str(i) for i in range(0,num_timesteps)]].T\n",
    "#         df_values.columns = df_values.columns.astype(str)\n",
    "#         df_values.reset_index(drop=True, inplace=True)\n",
    "#         df_values.columns = [str(i) for i in range(df_values.shape[1])]\n",
    "#         df_values.to_parquet(VALUE_DF_NAME + f'_{i_saved // SAVING_INTERVAL}.parquet', engine=\"fastparquet\", compression=\"snappy\")\n",
    "\n",
    "#         processed_lcs = []\n",
    "#         kepids = []\n",
    "#         planet_nums = []\n",
    "#         labels = []\n",
    "\n",
    "#         just_saved_flag = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eca3412",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>LABEL</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>69990</th>\n",
       "      <th>69991</th>\n",
       "      <th>69992</th>\n",
       "      <th>69993</th>\n",
       "      <th>69994</th>\n",
       "      <th>69995</th>\n",
       "      <th>69996</th>\n",
       "      <th>69997</th>\n",
       "      <th>69998</th>\n",
       "      <th>69999</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KEPID</th>\n",
       "      <th>PLANET_NUM</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9579641</th>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.312961</td>\n",
       "      <td>-0.095957</td>\n",
       "      <td>-0.319565</td>\n",
       "      <td>-0.261222</td>\n",
       "      <td>-0.074698</td>\n",
       "      <td>0.096192</td>\n",
       "      <td>-0.014046</td>\n",
       "      <td>-0.170279</td>\n",
       "      <td>0.002636</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11304958</th>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.021482</td>\n",
       "      <td>-0.139720</td>\n",
       "      <td>0.002269</td>\n",
       "      <td>0.045884</td>\n",
       "      <td>0.026515</td>\n",
       "      <td>0.082143</td>\n",
       "      <td>-0.297937</td>\n",
       "      <td>-0.324884</td>\n",
       "      <td>-0.022064</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11391957</th>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.083154</td>\n",
       "      <td>0.109080</td>\n",
       "      <td>-0.163022</td>\n",
       "      <td>0.291097</td>\n",
       "      <td>0.381881</td>\n",
       "      <td>-0.051684</td>\n",
       "      <td>0.220637</td>\n",
       "      <td>0.150476</td>\n",
       "      <td>-0.014663</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11403044</th>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.187118</td>\n",
       "      <td>-0.165793</td>\n",
       "      <td>-0.350305</td>\n",
       "      <td>0.107896</td>\n",
       "      <td>0.025164</td>\n",
       "      <td>-0.191581</td>\n",
       "      <td>0.024537</td>\n",
       "      <td>0.071433</td>\n",
       "      <td>0.192610</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 70001 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     LABEL         0         1         2         3         4  \\\n",
       "KEPID    PLANET_NUM                                                            \n",
       "9579641  1               0 -0.312961 -0.095957 -0.319565 -0.261222 -0.074698   \n",
       "11304958 1               0 -0.021482 -0.139720  0.002269  0.045884  0.026515   \n",
       "11391957 1               0 -0.083154  0.109080 -0.163022  0.291097  0.381881   \n",
       "11403044 1               0 -0.187118 -0.165793 -0.350305  0.107896  0.025164   \n",
       "\n",
       "                            5         6         7         8  ...  69990  \\\n",
       "KEPID    PLANET_NUM                                          ...          \n",
       "9579641  1           0.096192 -0.014046 -0.170279  0.002636  ...    NaN   \n",
       "11304958 1           0.082143 -0.297937 -0.324884 -0.022064  ...    NaN   \n",
       "11391957 1          -0.051684  0.220637  0.150476 -0.014663  ...    NaN   \n",
       "11403044 1          -0.191581  0.024537  0.071433  0.192610  ...    NaN   \n",
       "\n",
       "                     69991  69992  69993  69994  69995  69996  69997  69998  \\\n",
       "KEPID    PLANET_NUM                                                           \n",
       "9579641  1             NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "11304958 1             NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "11391957 1             NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "11403044 1             NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "\n",
       "                     69999  \n",
       "KEPID    PLANET_NUM         \n",
       "9579641  1             NaN  \n",
       "11304958 1             NaN  \n",
       "11391957 1             NaN  \n",
       "11403044 1             NaN  \n",
       "\n",
       "[4 rows x 70001 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#READING DF\n",
    "# df_values_loaded = pd.read_parquet(VALUE_DF_NAME + f'_final.parquet', engine=\"fastparquet\")\n",
    "# df_ids_loaded = pd.read_csv(ID_DF_NAME + f'_final.csv')\n",
    "# df_full = pd.concat([df_ids_loaded, df_values_loaded.T.reset_index(drop=True)], axis=1)\n",
    "# df_full.set_index(['KEPID','PLANET_NUM'], inplace=True)\n",
    "# df_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5621a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # SAVING DF\n",
    "lcs_df = pd.DataFrame(processed_lcs)\n",
    "identity_df = pd.DataFrame({'LABEL': labels, 'KEPID': kepids, 'PLANET_NUM' : planet_nums})\n",
    "#lcs_df = pd.concat((identity_df, lcs_df))\n",
    "\n",
    "identity_df.to_csv(ID_DF_NAME + f'_final.csv', index=False)\n",
    "lcs_df.columns = lcs_df.columns.astype(str)\n",
    "df_values = lcs_df[[str(i) for i in range(0,num_timesteps)]].T\n",
    "df_values.columns = df_values.columns.astype(str)\n",
    "df_values.reset_index(drop=True, inplace=True)\n",
    "df_values.columns = [str(i) for i in range(df_values.shape[1])]\n",
    "df_values.to_parquet(VALUE_DF_NAME + f'_final.parquet', engine=\"fastparquet\", compression=\"snappy\")\n",
    "\n",
    "\n",
    "json.dump({'label_map': label_map, 'next_code': next_code}, open(\"label_mapping.json\", \"w\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
